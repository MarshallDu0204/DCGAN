{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "53bhC-bWye8m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "6364d79a-d71b-403d-99db-8913c503a422"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JmJ-yP9zn2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import activations"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrBj4yFOtShX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_generator():\n",
        "    noise_shape = (100,)\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    model.add(layers.Dense(6 * 6 * 512, input_shape = noise_shape))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((6, 6, 512)))\n",
        "    \n",
        "    model.add(layers.Conv2DTranspose(256, (5, 5), strides = (2, 2), padding = 'same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides = (2, 2), padding = 'same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    \n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides = (2, 2), padding = 'same'))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(3, (5, 5), strides = (2, 2), padding = 'same', activation='tanh'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def build_discriminator():\n",
        "        \n",
        "    img_shape = (96, 96, 3)\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides = (2, 2), input_shape=img_shape, padding = 'same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides = (2, 2), input_shape=img_shape, padding = 'same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(256, (5, 5), strides = (2, 2), input_shape=img_shape, padding = 'same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(512, (5, 5), strides = (2, 2), input_shape=img_shape, padding = 'same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model\n",
        "\n",
        "def get_data(imgPath,dirNum):\n",
        "    basePath = '/content/drive/My Drive/Colab Notebooks/DCGAN-data/'\n",
        "    basePath = basePath + str(dirNum) + '/'\n",
        "    X = []\n",
        "    for path in tqdm(imgPath):\n",
        "        img = cv2.imread(basePath + path)\n",
        "        img = np.asarray(img)\n",
        "        img = cv2.resize(img,(96,96))\n",
        "        X.append(img)\n",
        "    return X\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "def gene_imgs(count,g):\n",
        "    noise = np.random.normal(0, 1, (count, 100))\n",
        "    return g.predict(noise)\n",
        "    \n",
        "def save_imgs(epoch,g):\n",
        "    r, c = 5, 5\n",
        "\n",
        "    imgs = gene_imgs(r*c,g)\n",
        "    imgs = 127.5 * imgs + 127.5\n",
        "\n",
        "    nindex, height, width, intensity = imgs.shape\n",
        "    nrows = nindex // c\n",
        "    assert nindex == nrows * c\n",
        "        \n",
        "    gallery = (imgs.reshape(nrows, c, height, width, intensity)\n",
        "                  .swapaxes(1, 2)\n",
        "                  .reshape(height * nrows, width * c, intensity))\n",
        "    path = '/content/drive/My Drive/Colab Notebooks/gantest' + '/gallery'\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    cv2.imwrite(path + f\"/{epoch}.jpg\", gallery)\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "generator_optimizer = tf.keras.optimizers.Adam(0.0002,0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(0.0002,0.5)\n",
        "checkpoint_dir = '/content/drive/My Drive/Colab Notebooks/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, 100])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "def train(dataset, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "\n",
        "        for image_batch in dataset:\n",
        "            train_step(image_batch)\n",
        "        save_imgs(epoch,generator)\n",
        "        if (epoch + 1) % 25 == 0:\n",
        "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "\n",
        "pathList = os.listdir('/content/drive/My Drive/Colab Notebooks/DCGAN-data/1/')\n",
        "X_train1 = get_data(pathList,1)\n",
        "pathList = os.listdir('/content/drive/My Drive/Colab Notebooks/DCGAN-data/2/')\n",
        "X_train2 = get_data(pathList,2)\n",
        "pathList = os.listdir('/content/drive/My Drive/Colab Notebooks/DCGAN-data/3/')\n",
        "X_train3 = get_data(pathList,3)\n",
        "pathList = os.listdir('/content/drive/My Drive/Colab Notebooks/DCGAN-data/4/')\n",
        "X_train4 = get_data(pathList,4)\n",
        "X_train = X_train1 + X_train2 + X_train3 + X_train4\n",
        "X_train = np.asarray(X_train)\n",
        "'''\n",
        "np.save('/content/drive/My Drive/Colab Notebooks/data.npy',X_train)  #save the data \n",
        "print('data_save')\n",
        "\n",
        "X_train = np.load('/content/drive/My Drive/Colab Notebooks/data.npy')   #preload the data \n",
        "print('data_load')\n",
        "'''\n",
        "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(X_train).batch(BATCH_SIZE)\n",
        "train(train_dataset,50)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}